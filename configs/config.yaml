# experiment name. defaults to name of training config
experiment: ''

# 'Apr 11 01:08'
# reward_weights = {
#     "r_base_pos": 0.0,
#     "r_ee_pos": 5.0,
#     "r_precision": 1.0,
#     "r_orient": -1.0,
#     "r_ctrl": {
#         'base': -0.2,
#         'arm': -1.0,
#     },
#     "r_collision": -15.0,
#     "r_constraint": -1.0,
#     "r_progress": 300.0,
#     "r_time": -1.0,
# }


# seed - set to -1 to choose random seed
seed: -1
# set to True for deterministic performance
torch_deterministic: False
## Device config
rl_device: ???
device_id: 0  # 'cuda:?', -1 for 'cpu'

# Disables viewer or camera visualization
viewer: True
imshow_cam: False
# RL Arguments
test: True # False, True
num_envs: 1 # This should be no more than 2x your CPUs (1x is recommended)
task: Tracing # Catching_TwoStage, Catching_OneStage, Tracking
# used to set checkpoint path
checkpoint_tracking: ''
checkpoint_catching: ''
checkpoint_tracing: ''
# checkpoint_tracing: 'assets/models/trace.pth'
# checkpoint_tracking: 'assets/models/track.pth'
# checkpoint_catching: 'assets/models/catch_two_stage.pth'


# make sure to set batch_size, minibatch_size according to num_envs in WireWalkerPPO.yaml
# self.batch_size = self.horizon_length * self.num_actors
# num_actors = num_envs
# default: 
# num_envs: 8
# minibatch_size: 512
# horizon_length: 64

# num_envs: 8 # This should be no more than 2x your CPUs (1x is recommended)
wire_eval: False

# wandb config
output_name: WireWalker
wandb_mode: "disabled"  # "online" | "offline" | "disabled"
wandb_entity: 'Your_username'
# wandb_project: 'RL_Dcmm_Track_Random'
wandb_project: 'RL_Dcmm_Catch_Random'

# set default task and default training config based on task
# currently we keep the same PPO settings for WireWalker
defaults:
  - train: WireWalkerPPO
  - hydra/job_logging: disabled

# set the directory where the output files get saved
hydra:
  output_subdir: null
  run:
    dir: .
